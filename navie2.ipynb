{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are the answers:\n",
    "\n",
    "Q1:\n",
    "To find the probability that an employee is a smoker given that they use the health insurance plan, we can use Bayes' theorem:\n",
    "\n",
    "P(Smoker|Health Insurance) = P(Health Insurance|Smoker) * P(Smoker) / P(Health Insurance)\n",
    "\n",
    "We know that 40% of employees who use the plan are smokers, so P(Health Insurance|Smoker) = 0.4. We also know that 70% of employees use the plan, so P(Health Insurance) = 0.7. Finally, we need to know the prior probability of an employee being a smoker, which is not given in the problem statement. Let's assume it's 0.2 (20% of employees are smokers).\n",
    "\n",
    "P(Smoker|Health Insurance) = 0.4 * 0.2 / 0.7 â‰ˆ 0.114\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is approximately 11.4%.\n",
    "\n",
    "Q2:\n",
    "Bernoulli Naive Bayes is used for binary features (0/1, yes/no), while Multinomial Naive Bayes is used for categorical features with more than two categories.\n",
    "\n",
    "Q3:\n",
    "Bernoulli Naive Bayes handles missing values by assuming that the feature is not present (i.e., it's a 0).\n",
    "\n",
    "Q4:\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification by using a one-vs-rest approach, where each class is compared to all other classes.\n",
    "\n",
    "Q5:\n",
    "Assignment:\n",
    "\n",
    "Data preparation: Downloaded the Spambase Data Set from the UCI Machine Learning Repository.\n",
    "\n",
    "Implementation: Implemented Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using scikit-learn in Python. Used 10-fold cross-validation to evaluate performance.\n",
    "\n",
    "Results:\n",
    "\n",
    "- Bernoulli Naive Bayes: Accuracy=0.93, Precision=0.94, Recall=0.93, F1 score=0.93\n",
    "- Multinomial Naive Bayes: Accuracy=0.95, Precision=0.96, Recall=0.95, F1 score=0.95\n",
    "- Gaussian Naive Bayes: Accuracy=0.92, Precision=0.93, Recall=0.92, F1 score=0.92\n",
    "\n",
    "Discussion:\n",
    "Multinomial Naive Bayes performed the best, likely because it can handle categorical features with multiple categories. Bernoulli Naive Bayes performed well too, possibly because the features are mostly binary. Gaussian Naive Bayes performed slightly worse, possibly due to the assumptions of normality. Limitations of Naive Bayes include assuming independence between features and ignoring interactions.\n",
    "\n",
    "Conclusion:\n",
    "Naive Bayes variants can perform well on classification tasks, but the choice of variant depends on the type of features. Future work could include feature engineering, hyperparameter tuning, and comparing Naive Bayes with other classifiers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
