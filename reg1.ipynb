{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are the answers:\n",
    "\n",
    "Q1:\n",
    "\n",
    "Simple linear regression involves modeling the relationship between a single independent variable and a dependent variable. For example, predicting the price of a house based on its size.\n",
    "\n",
    "Multiple linear regression involves modeling the relationship between multiple independent variables and a dependent variable. For example, predicting the price of a house based on its size, number of bedrooms, and location.\n",
    "\n",
    "Q2:\n",
    "\n",
    "The assumptions of linear regression include:\n",
    "\n",
    "- Linearity: The relationship between the independent and dependent variables is linear.\n",
    "- Independence: Each observation is independent of the others.\n",
    "- Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable.\n",
    "- Normality: The residuals are normally distributed.\n",
    "- No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "\n",
    "To check these assumptions, you can use various statistical tests and visualizations, such as scatterplots, residual plots, and variance inflation factor (VIF) calculations.\n",
    "\n",
    "Q3:\n",
    "\n",
    "The slope represents the change in the dependent variable for a one-unit change in the independent variable, while the intercept represents the value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "For example, in a model predicting the price of a house based on its size, the slope might be $100 per square foot, and the intercept might be $50,000. This means that for every additional square foot of size, the price increases by $100, and the base price of the house is $50,000.\n",
    "\n",
    "Q4:\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function in machine learning. It works by iteratively adjusting the model parameters to find the values that minimize the difference between the predicted and actual values.\n",
    "\n",
    "Q5:\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that allows for multiple independent variables. It models the relationship between multiple variables and a dependent variable using a linear equation.\n",
    "\n",
    "Q6:\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables are highly correlated with each other. This can lead to unstable estimates of the regression coefficients and inaccurate predictions.\n",
    "\n",
    "To detect multicollinearity, you can use VIF calculations or correlation matrices. To address it, you can remove one of the correlated variables, use regularization techniques, or use dimensionality reduction methods.\n",
    "\n",
    "Q7:\n",
    "\n",
    "Polynomial regression is a type of regression that models the relationship between the independent and dependent variables using a polynomial equation of degree n.\n",
    "\n",
    "For example, a quadratic regression model might predict the price of a house based on its size using a polynomial equation of degree 2.\n",
    "\n",
    "Q8:\n",
    "\n",
    "Polynomial regression can capture non-linear relationships between the variables, but it can also lead to overfitting and poor generalization performance.\n",
    "\n",
    "The advantages of polynomial regression include its ability to capture non-linear relationships and its flexibility in modeling different types of relationships. The disadvantages include overfitting and the need for careful selection of the degree of the polynomial.\n",
    "\n",
    "You would prefer to use polynomial regression when the relationship between the variables is non-linear, but you need to be careful not to overfit the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
