{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "Overfitting: Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts the performance of the model on new, unseen data. Consequences include poor generalization and high variance. It can be mitigated by techniques such as cross-validation, regularization, and using more training data.\n",
    "\n",
    "Underfitting: Underfitting occurs when a model is too simplistic to capture the underlying patterns in the training data. Consequences include poor performance on both training and test data due to high bias. It can be mitigated by using more complex models, adding more features, or reducing regularization.\n",
    "\n",
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "To reduce overfitting:\n",
    "\n",
    "Use regularization techniques (e.g., L1/L2 regularization, dropout).\n",
    "Use cross-validation to evaluate model performance.\n",
    "Gather more training data if possible.\n",
    "Simplify the model architecture or reduce the number of features.\n",
    "Use techniques like early stopping during training.\n",
    "\n",
    "\n",
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test sets.\n",
    "\n",
    "Scenarios of underfitting:\n",
    "\n",
    "Using a linear model for a complex nonlinear problem.\n",
    "Using insufficient data to train the model.\n",
    "Removing too many features or using insufficiently complex models.\n",
    "\n",
    "\n",
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "\n",
    "Bias: Bias measures how far off the predictions are from the actual values. High bias models are too simplistic and underfit the data.\n",
    "Variance: Variance measures the sensitivity of the model to changes in the training data. High variance models are complex and overfit the data.\n",
    "The bias-variance tradeoff refers to the balance between bias and variance:\n",
    "\n",
    "Increasing model complexity reduces bias but increases variance.\n",
    "Decreasing model complexity increases bias but reduces variance.\n",
    "The goal is to find the right balance to minimize both bias and variance for optimal model performance on unseen data.\n",
    "\n",
    "\n",
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "Overfitting detection: Use techniques such as:\n",
    "\n",
    "Cross-validation: Compare training and validation performance.\n",
    "Learning curves: Plot performance metrics against training set size.\n",
    "Validation curves: Plot performance metrics against model complexity parameters.\n",
    "Underfitting detection: Look for signs such as:\n",
    "\n",
    "Poor performance on training and test sets.\n",
    "Consistently low accuracy or high error rates.\n",
    "Simple model unable to capture patterns in the data.\n",
    "\n",
    "\n",
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "Bias: High bias models are too simplistic and underfit the data. Examples include linear regression on nonlinear data or shallow decision trees on complex data. They have poor performance on both training and test sets.\n",
    "\n",
    "Variance: High variance models are complex and overfit the data. Examples include deep neural networks with insufficient regularization or decision trees with many levels. They have good performance on the training set but poor performance on the test set.\n",
    "\n",
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "\n",
    "Regularization: Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models.\n",
    "Common regularization techniques include:\n",
    "\n",
    "L1 Regularization (Lasso): Adds the absolute value of the coefficients as a penalty.\n",
    "L2 Regularization (Ridge): Adds the square of the coefficients as a penalty.\n",
    "Elastic Net Regularization: Combines L1 and L2 penalties.\n",
    "Dropout: Randomly drops units (along with their connections) during training to prevent over-reliance on certain neurons.\n",
    "These techniques help in controlling the model's complexity and improving its generalization to unseen data by reducing the variance without significantly increasing bias."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
