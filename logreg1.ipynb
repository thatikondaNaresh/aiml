{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are the answers:\n",
    "\n",
    "Q1:\n",
    "\n",
    "Linear regression predicts a continuous output variable, while logistic regression predicts a binary output variable. Logistic regression is more appropriate when the output variable is binary or categorical, such as predicting whether a customer will churn (yes/no) or whether a credit card transaction is fraudulent (yes/no).\n",
    "\n",
    "Example: A company wants to predict whether a customer will respond to a promotion based on their demographic data. Logistic regression would be more appropriate because the output variable is binary (responded or not responded).\n",
    "\n",
    "Q2:\n",
    "\n",
    "The cost function used in logistic regression is the log loss or cross-entropy loss. The goal is to minimize the log loss by adjusting the model's parameters to maximize the likelihood of correctly predicting the output variable.\n",
    "\n",
    "Optimization techniques such as gradient descent, Adam, or RMSProp are used to minimize the log loss.\n",
    "\n",
    "Q3:\n",
    "\n",
    "Regularization in logistic regression adds a penalty term to the cost function to prevent overfitting. L1 (Lasso) and L2 (Ridge) regularization are common techniques used to reduce the magnitude of model coefficients and prevent overfitting.\n",
    "\n",
    "Regularization helps prevent overfitting by adding a penalty term to the cost function, which discourages large coefficients.\n",
    "\n",
    "Q4:\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve plots the true positive rate against the false positive rate at different thresholds. It's used to evaluate the model's performance and determine the optimal threshold for classification.\n",
    "\n",
    "A higher ROC curve indicates better model performance.\n",
    "\n",
    "Q5:\n",
    "\n",
    "Common techniques for feature selection in logistic regression include:\n",
    "\n",
    "- Mutual information\n",
    "- Correlation analysis\n",
    "- Recursive feature elimination (RFE)\n",
    "- LASSO regression\n",
    "\n",
    "These techniques help improve the model's performance by selecting relevant features and reducing the dimensionality of the data.\n",
    "\n",
    "Q6:\n",
    "\n",
    "To handle imbalanced datasets in logistic regression, strategies such as:\n",
    "\n",
    "- Oversampling the minority class\n",
    "- Undersampling the majority class\n",
    "- Synthetic data generation (e.g., SMOTE)\n",
    "- Class weights\n",
    "- Anomaly detection\n",
    "\n",
    "can be used to balance the classes and improve the model's performance.\n",
    "\n",
    "Q7:\n",
    "\n",
    "Common issues and challenges in logistic regression include:\n",
    "\n",
    "- Multicollinearity: Use regularization techniques or feature selection to reduce correlation between independent variables.\n",
    "- Overfitting: Use regularization techniques or early stopping to prevent overfitting.\n",
    "- Class imbalance: Use strategies mentioned earlier to balance the classes.\n",
    "- Non-linear relationships: Use non-linear models such as decision trees or random forests.\n",
    "\n",
    "Addressing these issues can improve the model's performance and generalization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
